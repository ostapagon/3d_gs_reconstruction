{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ca995a8-2112-45b9-bc17-614e21a41475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/app')\n",
    "# sys.path.append('/app/submodules')\n",
    "sys.path.append('/app/submodules/Depth-Anything/metric_depth/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97d4f0ba-e8cc-4a66-93ba-b5962443c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c93cd2c-88d0-494c-927c-d2cb7e9e28ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import open3d as o3d\n",
    "from tqdm import tqdm\n",
    "from zoedepth.models.builder import build_model\n",
    "from zoedepth.utils.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66de5b58-d7a3-4541-96b3-254bbcbb0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings\n",
    "FL = 715.0873\n",
    "FY = 256 * 0.6\n",
    "FX = 256 * 0.6\n",
    "NYU_DATA = False\n",
    "FINAL_HEIGHT = 256\n",
    "FINAL_WIDTH = 256\n",
    "INPUT_DIR = './my_test/input'\n",
    "OUTPUT_DIR = './my_test/output'\n",
    "DATASET = 'nyu' # Lets not pick a fight with the model's dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "defbd007-1ea5-422f-8699-3918e8e5dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(model):\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    image_paths = glob.glob(os.path.join(INPUT_DIR, '*.png')) + glob.glob(os.path.join(INPUT_DIR, '*.jpg'))\n",
    "    for image_path in tqdm(image_paths, desc=\"Processing Images\"):\n",
    "        try:\n",
    "            color_image = Image.open(image_path).convert('RGB')\n",
    "            original_width, original_height = color_image.size\n",
    "            image_tensor = transforms.ToTensor()(color_image).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            pred = model(image_tensor, dataset=DATASET)\n",
    "            if isinstance(pred, dict):\n",
    "                pred = pred.get('metric_depth', pred.get('out'))\n",
    "            elif isinstance(pred, (list, tuple)):\n",
    "                pred = pred[-1]\n",
    "            pred = pred.squeeze().detach().cpu().numpy()\n",
    "\n",
    "            # Resize color image and depth to final size\n",
    "            resized_color_image = color_image.resize((FINAL_WIDTH, FINAL_HEIGHT), Image.LANCZOS)\n",
    "            resized_pred = Image.fromarray(pred).resize((FINAL_WIDTH, FINAL_HEIGHT), Image.NEAREST)\n",
    "\n",
    "            focal_length_x, focal_length_y = (FX, FY) if not NYU_DATA else (FL, FL)\n",
    "            x, y = np.meshgrid(np.arange(FINAL_WIDTH), np.arange(FINAL_HEIGHT))\n",
    "            x = (x - FINAL_WIDTH / 2) / focal_length_x\n",
    "            y = (y - FINAL_HEIGHT / 2) / focal_length_y\n",
    "            z = np.array(resized_pred)\n",
    "            points = np.stack((np.multiply(x, z), np.multiply(y, z), z), axis=-1).reshape(-1, 3)\n",
    "            colors = np.array(resized_color_image).reshape(-1, 3) / 255.0\n",
    "\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(points)\n",
    "            pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "            o3d.io.write_point_cloud(os.path.join(OUTPUT_DIR, os.path.splitext(os.path.basename(image_path))[0] + \".ply\"), pcd)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "119d6357-5c37-44be-9179-b92f9ce56801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_name, pretrained_resource):\n",
    "    config = get_config(model_name, \"eval\", DATASET)\n",
    "    config.pretrained_resource = pretrained_resource\n",
    "    model = build_model(config).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    process_images(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e518dea5-4c01-4383-83cf-8ea3f28b2821",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    model: str = 'zoedepth'\n",
    "    pretrained_resource: str = '/app/checkpoints/depth-anything/depth_anything_metric_depth_indoor.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83a07ea2-10fa-4f8a-8a4c-2c73b95172fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/app/notebooks/../torchhub/facebookresearch_dinov2_main/hubconf.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_name, pretrained_resource)\u001b[0m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config(model_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m, DATASET)\n\u001b[1;32m      3\u001b[0m config\u001b[38;5;241m.\u001b[39mpretrained_resource \u001b[38;5;241m=\u001b[39m pretrained_resource\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m process_images(model)\n",
      "File \u001b[0;32m/app/submodules/Depth-Anything/metric_depth/zoedepth/models/builder.py:51\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no get_version function.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/submodules/Depth-Anything/metric_depth/zoedepth/models/zoedepth/zoedepth_v1.py:264\u001b[0m, in \u001b[0;36mZoeDepth.build_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_from_config\u001b[39m(config):\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mZoeDepth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/submodules/Depth-Anything/metric_depth/zoedepth/models/zoedepth/zoedepth_v1.py:253\u001b[0m, in \u001b[0;36mZoeDepth.build\u001b[0;34m(midas_model_type, pretrained_resource, use_pretrained_midas, train_midas, freeze_midas_bn, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(midas_model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDPT_BEiT_L_384\u001b[39m\u001b[38;5;124m\"\u001b[39m, pretrained_resource\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_pretrained_midas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, train_midas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, freeze_midas_bn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# core = MidasCore.build(midas_model_type=midas_model_type, use_pretrained_midas=use_pretrained_midas,\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m#                        train_midas=train_midas, fetch_features=True, freeze_bn=freeze_midas_bn, **kwargs)\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     core \u001b[38;5;241m=\u001b[39m \u001b[43mDepthAnythingCore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmidas_model_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmidas_model_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_pretrained_midas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pretrained_midas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtrain_midas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_midas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze_bn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_midas_bn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     model \u001b[38;5;241m=\u001b[39m ZoeDepth(core, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_resource:\n",
      "File \u001b[0;32m/app/submodules/Depth-Anything/metric_depth/zoedepth/models/base_models/depth_anything.py:339\u001b[0m, in \u001b[0;36mDepthAnythingCore.build\u001b[0;34m(midas_model_type, train_midas, use_pretrained_midas, fetch_features, freeze_bn, force_keep_ar, force_reload, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m DepthAnythingCore\u001b[38;5;241m.\u001b[39mparse_img_size(kwargs)\n\u001b[1;32m    337\u001b[0m img_size \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;241m384\u001b[39m, \u001b[38;5;241m384\u001b[39m])\n\u001b[0;32m--> 339\u001b[0m depth_anything \u001b[38;5;241m=\u001b[39m \u001b[43mDPT_DINOv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_clstoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoints/depth_anything_vitl14.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    342\u001b[0m depth_anything\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "File \u001b[0;32m/app/submodules/Depth-Anything/metric_depth/zoedepth/models/base_models/dpt_dinov2/dpt.py:140\u001b[0m, in \u001b[0;36mDPT_DINOv2.__init__\u001b[0;34m(self, encoder, features, use_bn, out_channels, use_clstoken)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28msuper\u001b[39m(DPT_DINOv2, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    138\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../torchhub/facebookresearch_dinov2_main\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdinov2_\u001b[39;49m\u001b[38;5;132;43;01m{:}\u001b[39;49;00m\u001b[38;5;124;43m14\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mqkv\u001b[38;5;241m.\u001b[39min_features\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth_head \u001b[38;5;241m=\u001b[39m DPTHead(dim, features, use_bn, out_channels\u001b[38;5;241m=\u001b[39mout_channels, use_clstoken\u001b[38;5;241m=\u001b[39muse_clstoken)\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/torch/hub.py:558\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    555\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    556\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 558\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/torch/hub.py:584\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(hubconf_dir):\n\u001b[1;32m    583\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[0;32m--> 584\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m    587\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/torch/hub.py:98\u001b[0m, in \u001b[0;36m_import_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m     96\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, Loader)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1016\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1073\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/app/notebooks/../torchhub/facebookresearch_dinov2_main/hubconf.py'"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "main(args.model, args.pretrained_resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199b17d-c0d3-4c89-aac0-3680e295d58f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
